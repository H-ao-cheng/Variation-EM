{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:16:04.483741Z",
     "start_time": "2024-12-17T12:16:04.226789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from doc.pycurl.examples.quickstart.response_headers import encoding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties"
   ],
   "id": "d6197b125d3732c7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T12:16:36.660924Z",
     "start_time": "2024-12-17T12:16:36.588544Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.special import psi, polygamma\n",
    "\n",
    "# 初始化参数\n",
    "def initialize_parameters(documents, vocab_size, num_topics):\n",
    "    phi = np.random.rand(len(documents), vocab_size, num_topics)\n",
    "    gamma = np.random.rand(len(documents), num_topics)\n",
    "    lambda_ = np.random.rand(num_topics, vocab_size)\n",
    "    alpha = np.ones(num_topics)\n",
    "    eta = np.ones(vocab_size)\n",
    "    return phi, gamma, lambda_, alpha, eta\n",
    "\n",
    "# 更新phi\n",
    "def update_phi(d, n, k, documents, phi, gamma, lambda_):\n",
    "    sum1 = 0\n",
    "    for i in range(vocab_size):\n",
    "        sum1 += documents[d][n][i] * (psi(lambda_[k][i]) - psi(np.sum(lambda_[k])))\n",
    "\n",
    "    sum2 = psi(gamma[d][k]) - psi(np.sum(gamma[d]))\n",
    "\n",
    "    phi_dnk = np.exp(sum1 + sum2)\n",
    "    return phi_dnk\n",
    "\n",
    "# 更新gamma\n",
    "def update_gamma(k, d, alpha, phi):\n",
    "    gamma_kd = alpha[k] + np.sum(phi[d][:, k])\n",
    "    return gamma_kd\n",
    "\n",
    "# 更新lambda_\n",
    "def update_lambda(k, i, eta, phi, documents):\n",
    "    lambda_ki = eta[i] + np.sum([np.sum([phi[d][n][k] * documents[d][n][i] for n in range(len(documents[d]))]) for d in range(len(documents))])\n",
    "    return lambda_ki\n",
    "\n",
    "# 更新alpha和eta\n",
    "def update_alpha_eta(alpha, eta, phi, gamma, lambda_, documents, vocab_size, num_topics):\n",
    "    grad_alpha = compute_grad_alpha(phi, gamma, lambda_)\n",
    "    hessian_alpha = compute_hessian_alpha(phi, gamma, lambda_)\n",
    "    new_alpha = alpha + grad_alpha / hessian_alpha\n",
    "\n",
    "    grad_eta = compute_grad_eta(phi, lambda_)\n",
    "    hessian_eta = compute_hessian_eta(phi, lambda_)\n",
    "    new_eta = eta + grad_eta / hessian_eta\n",
    "\n",
    "    return new_alpha, new_eta\n",
    "\n",
    "# 计算ELBO\n",
    "def compute_elbo(documents, vocab_size, num_topics, alpha, eta, phi, gamma, lambda_):\n",
    "    elbo_part1 = 0\n",
    "    for k in range(num_topics):\n",
    "        elbo_part1 += np.sum((psi(lambda_[k]) - psi(np.sum(lambda_[k]))) * (lambda_[k] - eta))\n",
    "\n",
    "    elbo_part2 = 0\n",
    "    for d in range(len(documents)):\n",
    "        for n in range(len(documents[d])):\n",
    "            for k in range(num_topics):\n",
    "                elbo_part2 += phi[d][n][k] * (psi(gamma[d][k]) - psi(np.sum(gamma[d])))\n",
    "\n",
    "    elbo_part3 = 0\n",
    "    for d in range(len(documents)):\n",
    "        for k in range(num_topics):\n",
    "            elbo_part3 += (gamma[d][k] - alpha[k]) * (psi(gamma[d][k]) - psi(np.sum(gamma[d])))\n",
    "\n",
    "    elbo_part4 = 0\n",
    "    for d in range(len(documents)):\n",
    "        for n in range(len(documents[d])):\n",
    "            for k in range(num_topics):\n",
    "                elbo_part4 += phi[d][n][k] * np.log(lambda_[k][documents[d][n]])\n",
    "\n",
    "    elbo_part5 = 0\n",
    "    for k in range(num_topics):\n",
    "        elbo_part5 -= np.sum((lambda_[k] - 1) * (psi(lambda_[k]) - psi(np.sum(lambda_[k]))))\n",
    "\n",
    "    elbo_part6 = 0\n",
    "    for d in range(len(documents)):\n",
    "        for n in range(len(documents[d])):\n",
    "            for k in range(num_topics):\n",
    "                elbo_part6 -= phi[d][n][k] * (psi(gamma[d][k]) - psi(np.sum(gamma[d])))\n",
    "\n",
    "    elbo_part7 = 0\n",
    "    for d in range(len(documents)):\n",
    "        for k in range(num_topics):\n",
    "            elbo_part7 -= (gamma[d][k] - alpha[k]) * (psi(gamma[d][k]) - psi(np.sum(gamma[d])))\n",
    "\n",
    "    elbo = elbo_part1 + elbo_part2 + elbo_part3 + elbo_part4 + elbo_part5 + elbo_part6 + elbo_part7\n",
    "    return elbo\n",
    "\n",
    "# 计算 $\\alpha$ 的一阶导数\n",
    "def compute_grad_alpha(phi, gamma, lambda_):\n",
    "    M = len(gamma)\n",
    "    K = len(gamma[0])\n",
    "\n",
    "    grad_alpha = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        sum_alpha = np.sum(gamma[:, k])\n",
    "        grad_alpha[k] = M * (polygamma(0, sum_alpha) - psi(gamma[:, k]))\n",
    "\n",
    "        for d in range(M):\n",
    "            sum_gamma_d = np.sum(gamma[d])\n",
    "            grad_alpha[k] += psi(gamma[d][k]) - psi(sum_gamma_d)\n",
    "\n",
    "    return grad_alpha\n",
    "\n",
    "# 计算 $\\alpha$ 的二阶导数\n",
    "def compute_hessian_alpha(phi, gamma, lambda_):\n",
    "    M = len(gamma)\n",
    "    K = len(gamma[0])\n",
    "\n",
    "    hessian_alpha = np.zeros((K, K))\n",
    "    for k in range(K):\n",
    "        for j in range(K):\n",
    "            sum_alpha = np.sum(gamma[:, k])\n",
    "            if k == j:\n",
    "                hessian_alpha[k, j] = M * (polygamma(1, sum_alpha) - polygamma(1, gamma[:, k]))\n",
    "            else:\n",
    "                hessian_alpha[k, j] = M * polygamma(1, sum_alpha)\n",
    "\n",
    "    return hessian_alpha\n",
    "\n",
    "# 计算 $\\eta$ 的一阶导数\n",
    "def compute_grad_eta(phi, lambda_):\n",
    "    V = lambda_.shape[1]\n",
    "    K = lambda_.shape[0]\n",
    "\n",
    "    grad_eta = np.zeros(V)\n",
    "    for i in range(V):\n",
    "        sum_eta = np.sum(lambda_[:, i])\n",
    "        grad_eta[i] = K * (polygamma(0, sum_eta) - psi(lambda_[:, i]))\n",
    "\n",
    "        for k in range(K):\n",
    "            sum_lambda_k = np.sum(lambda_[k])\n",
    "            grad_eta[i] += psi(lambda_[k][i]) - psi(sum_lambda_k)\n",
    "\n",
    "    return grad_eta\n",
    "\n",
    "# 计算 $\\eta$ 的二阶导数\n",
    "def compute_hessian_eta(phi, lambda_):\n",
    "    V = lambda_.shape[1]\n",
    "    K = lambda_.shape[0]\n",
    "\n",
    "    hessian_eta = np.zeros((V, V))\n",
    "    for i in range(V):\n",
    "        for j in range(V):\n",
    "            sum_eta = np.sum(lambda_[:, i])\n",
    "            if i == j:\n",
    "                hessian_eta[i, j] = K * (polygamma(1, sum_eta) - polygamma(1, lambda_[:, i]))\n",
    "            else:\n",
    "                hessian_eta[i, j] = K * polygamma(1, sum_eta)\n",
    "\n",
    "    return hessian_eta\n",
    "\n",
    "# 检查是否收敛\n",
    "def all_converged(phi, gamma, lambda_, alpha, eta, prev_alpha, prev_eta, elbos, param_tol=1e-3, elbo_tol=1e-3, patience=5):\n",
    "    # 检查参数变化是否小于阈值\n",
    "    alpha_diff = np.linalg.norm(alpha - prev_alpha)\n",
    "    eta_diff = np.linalg.norm(eta - prev_eta)\n",
    "    param_converged = alpha_diff < param_tol and eta_diff < param_tol\n",
    "\n",
    "    # 检查ELBO的变化是否小于阈值\n",
    "    if len(elbos) < patience + 1:\n",
    "        elbo_converged = False\n",
    "    else:\n",
    "        recent_gains = [elbos[-i] - elbos[-i-1] for i in range(1, patience+1)]\n",
    "        max_gain = max(recent_gains)\n",
    "        elbo_converged = max_gain < elbo_tol\n",
    "\n",
    "    return param_converged and elbo_converged\n",
    "\n",
    "# 主函数\n",
    "def lda_variational_em(documents, vocab_size, num_topics, alpha, eta):\n",
    "    phi, gamma, lambda_, alpha, eta = initialize_parameters(documents, vocab_size, num_topics)\n",
    "    elbos = []\n",
    "    prev_alpha = alpha.copy()\n",
    "    prev_eta = eta.copy()\n",
    "\n",
    "    while True:\n",
    "        # E步迭代循环\n",
    "        for d in range(len(documents)):\n",
    "            for n in range(len(documents[d])):\n",
    "                for k in range(num_topics):\n",
    "                    phi[d][n][k] = update_phi(d, n, k, documents, phi, gamma, lambda_)\n",
    "                    phi[d][n] /= np.sum(phi[d][n])\n",
    "\n",
    "        # 更新gamma\n",
    "        for d in range(len(documents)):\n",
    "            for k in range(num_topics):\n",
    "                gamma[d][k] = update_gamma(k, d, alpha, phi)\n",
    "\n",
    "        # 更新lambda_\n",
    "        for k in range(num_topics):\n",
    "            for i in range(vocab_size):\n",
    "                lambda_[k][i] = update_lambda(k, i, eta, phi, documents)\n",
    "\n",
    "        # 更新alpha和eta\n",
    "        alpha, eta = update_alpha_eta(alpha, eta, phi, gamma, lambda_, documents, vocab_size, num_topics)\n",
    "\n",
    "        # 计算ELBO\n",
    "        elbo = compute_elbo(documents, vocab_size, num_topics, alpha, eta, phi, gamma, lambda_)\n",
    "        elbos.append(elbo)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:16:54.604496Z",
     "start_time": "2024-12-17T12:16:54.600166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('./nlp_test1.txt', 'r', encoding='utf-8') as f3:\n",
    "    res1 = f3.read()\n",
    "\n",
    "print(res1)"
   ],
   "id": "16ba81a88334fcdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:17:08.646559Z",
     "start_time": "2024-12-17T12:17:08.642552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('./nlp_test3.txt', 'r', encoding='utf-8') as f4:\n",
    "    res2 = f4.read()\n",
    "print(res2)"
   ],
   "id": "cca9338fa851c3e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:21:34.542527Z",
     "start_time": "2024-12-17T12:21:34.537291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 文本预处理函数\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 读取并预处理文件内容\n",
    "def load_and_preprocess_files(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            processed_text = preprocess_text(text)\n",
    "            documents.append(processed_text)\n",
    "    return documents\n",
    "\n",
    "# 创建文档-词项矩阵\n",
    "def create_document_term_matrix(documents, vocab_size=None):\n",
    "    vectorizer = CountVectorizer(max_features=vocab_size)\n",
    "    dtm = vectorizer.fit_transform(documents).toarray()\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    return dtm, vocabulary\n",
    "\n",
    "# 初始化参数\n",
    "def initialize_parameters(dtm, num_topics):\n",
    "    phi = np.random.rand(len(dtm), dtm.shape[1], num_topics)\n",
    "    gamma = np.random.rand(len(dtm), num_topics)\n",
    "    lambda_ = np.random.rand(num_topics, dtm.shape[1])\n",
    "    alpha = np.ones(num_topics)\n",
    "    eta = np.ones(dtm.shape[1])\n",
    "    return phi, gamma, lambda_, alpha, eta\n"
   ],
   "id": "ad39eb7b169afefd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:46:00.728015Z",
     "start_time": "2024-12-17T12:46:00.696705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 从文件导入停用词表\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return set(stopwords)  # 使用集合提高查找效率\n",
    "\n",
    "# 文本预处理函数（使用 jieba 进行分词）\n",
    "def preprocess_text(text, stop_words):\n",
    "    words = jieba.lcut(text)  # 使用 jieba 进行分词\n",
    "    filtered_words = [word for word in words if word.strip() and word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 读取并预处理文件内容\n",
    "def load_and_preprocess_files(file_paths, stop_words):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            processed_text = preprocess_text(text, stop_words)\n",
    "            documents.append(processed_text)\n",
    "    return documents\n",
    "\n",
    "# 创建文档-词项矩阵\n",
    "def create_document_term_matrix(documents, vocab_size=None):\n",
    "    vectorizer = CountVectorizer(max_features=vocab_size, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    dtm = vectorizer.fit_transform(documents).toarray()\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    return dtm, vocabulary\n",
    "\n",
    "# 示例应用\n",
    "stpwrdpath = \"stop_words.txt\"\n",
    "stop_words = load_stopwords(stpwrdpath)\n",
    "file_paths = ['./nlp_test1.txt', './nlp_test3.txt']\n",
    "documents = load_and_preprocess_files(file_paths, stop_words)\n",
    "dtm, vocabulary = create_document_term_matrix(documents)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Document-Term Matrix:\\n\", dtm)"
   ],
   "id": "a87448a90225b000",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa1 in position 6: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# 示例应用\u001B[39;00m\n\u001B[0;32m     34\u001B[0m stpwrdpath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_words.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 35\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m load_stopwords(stpwrdpath)\n\u001B[0;32m     36\u001B[0m file_paths \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./nlp_test1.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./nlp_test3.txt\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     37\u001B[0m documents \u001B[38;5;241m=\u001B[39m load_and_preprocess_files(file_paths, stop_words)\n",
      "Cell \u001B[1;32mIn[13], line 7\u001B[0m, in \u001B[0;36mload_stopwords\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_stopwords\u001B[39m(file_path):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m----> 7\u001B[0m         stopwords \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mset\u001B[39m(stopwords)\n",
      "File \u001B[1;32m<frozen codecs>:322\u001B[0m, in \u001B[0;36mdecode\u001B[1;34m(self, input, final)\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xa1 in position 6: invalid start byte"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:30:21.915103Z",
     "start_time": "2024-12-17T12:30:21.899076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [res1, res2]\n",
    "cntVector = CountVectorizer(stop_words=stpwrdlst)\n",
    "cntTf = cntVector.fit_transform(corpus)\n",
    "print(cntTf)\n",
    "corpus = [res1,res2]\n",
    "vector = TfidfVectorizer(stop_words=stpwrdlst)\n",
    "tfidf = vector.fit_transform(corpus)\n",
    "print (tfidf)"
   ],
   "id": "e18d993eab9d964f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhaochenghao\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\zhaochenghao\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n",
      "C:\\Users\\zhaochenghao\\anaconda3\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\zhaochenghao\\anaconda3\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stpwrdlst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m corpus \u001B[38;5;241m=\u001B[39m [res1, res2]\n\u001B[1;32m----> 2\u001B[0m cntVector \u001B[38;5;241m=\u001B[39m CountVectorizer(stop_words\u001B[38;5;241m=\u001B[39mstpwrdlst)\n\u001B[0;32m      3\u001B[0m cntTf \u001B[38;5;241m=\u001B[39m cntVector\u001B[38;5;241m.\u001B[39mfit_transform(corpus)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(cntTf)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'stpwrdlst' is not defined"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
